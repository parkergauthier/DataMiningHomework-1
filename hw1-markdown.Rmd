---
title: 'Data Mining HW1: John Walkington, Karlo Vlahek, Parker Gauthier'
output: md_document
---

```{r include=FALSE}
setwd("~/School/UT/StatLearning/csvfiles")

library(tidyverse)
library(ggplot2)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)

olympics = read.csv("olympics_top20.csv", header = TRUE)
ABIA = read.csv('ABIA.csv', header = TRUE)
billboard = read.csv("billboard.csv", header = TRUE)
sclass = read.csv("sclass.csv", header = TRUE)
```

## 1) Data visualization: flights at ABIA

```{r include=FALSE}
airlines = c('AA',"UA","DL","WN","B6")

avg_hourly_delay = ABIA %>%
  filter(UniqueCarrier==airlines) %>% 
  mutate(DepHour = floor(DepTime/100)) %>% 
  group_by(DepHour, UniqueCarrier) %>% 
  summarise(avgdelay=mean(DepDelay))

airline_delays = ggplot(avg_hourly_delay) +
  geom_line(aes(y=avgdelay, x = DepHour, color=UniqueCarrier)) +
  labs(title ="Austin-Bergstrom Daily Average Departure Delays in 2008",
       y= "Average Delay (in Minutes)",
       x= "Hour of Day",
       color="Carriers") +
  scale_color_hue(labels= c("American Airlines", "JetBlue", 
  "Delta Airlines","United Airlines","Southwest Airlines"))
```

```{r Airline Delays (1), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
airline_delays
```

How did departures for Austin-Bergstrom International Airport vary in 2008? According to this chart, across all airlines, the greatest average departure delays occur in the morning, with Southwest delays being the most severe.  If you're flying United, you want to avoid early afternoon departures, as there is a huge spike in average delays in the hours between 11:00 am and 4:00 pm. 

## 2) Wrangling the Billboard Top 100

### Part A:

```{r Top 10 Songs Since 1958 (2A), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
billboard1 = billboard %>% 
  filter(year>1958) %>% 
  select(performer, song, year,week, week_position)

top10 = billboard1 %>% 
  group_by(performer,song) %>% 
  summarize(count=n()) %>% 
  arrange(desc(count)) %>% 
  head(10)

knitr::kable(top10,col.names = c("Performers", "Songs", "Total # of Weeks on Billboard"),
             caption = "Top 10 Most Popular Songs since 1958")
```

### Part B:

```{r Musical Diversity (2B), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
billboardfilter = billboard %>%
  filter(between(year,1959,2020)) %>%
  group_by(year) %>%
  count(song) %>%
  count(year)

ggplot(data = billboardfilter, aes(x = year, y = n)) +
  geom_line() +
  labs(title = "Song Diversity by Year", x = "Year", y = "Number of Unique Songs")
```

Song diversity on the Billboard Top 100 peaked in the mid-1960's, then steadily dropped until it hit a low in the early 2000's.  Since the early 2000's, however, song diversity has increased sharply, with almost-record high numbers for 2020.

### Part C:

```{r 19 Artists that are Built Different (2C), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
ten_week_hit = billboard %>% 
  filter(weeks_on_chart==10) %>% 
  group_by(performer) %>% 
  summarize(count=n()) %>% 
  filter(count>=30) %>% 
  arrange(desc(count))

ggplot(ten_week_hit) +
  geom_col(aes(fct_reorder(performer, count), count, color = performer)) +
  coord_flip() +
  labs(title = "Number of Songs on Billboard's Top 100
       For At Least 10 Weeks by Top Artists*", x = "Performer", y = "Number of Songs", caption = "*Data was filtered for artists who had at least 30 songs that appeared for at least 10 weeks.") +
  theme(legend.position = "none")
```

As shown above, there are 19 artists who had at least 30 songs on Billboard's Top 100 for 10 weeks or more. We can see that Elton John dominates this category, with the most songs of this classification.

## 3) Wrangling the Olympics

### Part A:

```{r 95th Percentile for Female Competitors (3A), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
height = olympics %>%
  filter(sex == "F") %>%
  group_by(event) %>%
  summarize(topheight = quantile(height, .95)) %>%
  arrange(desc(topheight))
height
```

Female competitors across all athletic events in the top 20 Olympic sports. The Women's Basketball team has the highest (and therefore tallest) percentiles among all female sports. The Women's Triple Jump has the lowest.

### Part B:

```{r Greatest variabilty in competitor heights among Womens events (3B), echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
variation = olympics %>%
  filter(sex == "F") %>%
  group_by(event) %>%
  summarize(height_variation = sd(height)) %>%
  arrange(desc(height_variation)) %>%
  head(1)
variation
```

Rowing Women's Coxed Fours has the most variation in height among competitors, with a standard deviation of 10.9 cm.

### Part C:

```{r Average Age of Olympic Swimmers (3C), echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
average_age = olympics %>%
  filter(sport == "Swimming") %>%
  group_by(year, sex) %>%
  summarise(average_age = mean(age))

ggplot(data = average_age) +
  geom_line(aes(x=year, y=average_age, color=sex)) +
  labs(title = "Average Age of Olympic Swimming Medalists by Year", x = "Year", y = "Average age (years)")
```

The average age of Olympic swimming medalists spiked in 1924, but dropped to an average age of about 20 for men and 17 for women until the 1980s, when there began a gradual increase in age for both sexes.

## 4.) K-nearest Neighbors

```{r Preparation, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
###
#Question 4
###

# PREPARATION
library(tidyverse)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)


# CLEANING/SEPARATING THE DATASET
sclass350 = sclass %>%
  select(trim, mileage, price) %>%
  filter(trim == "350")

sclass65 = sclass %>%
  select(trim, mileage, price) %>%
  filter(trim == "65 AMG")
```

### Trim 350 Analysis
#### Making predictions

```{r Splitting Trim 350s for KNN, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# SPLITTING TRIM 350'S DATASET FOR KNN
sclass350_split = initial_split(sclass350, prop = 0.80)
sclass350_train = training(sclass350_split)
sclass350_test = testing(sclass350_split)



# FOR DIFFERENT LEVELS OF K, MAKING PREDICTIONS ON THE TESTING SET AND CALCULATING THE RMSE #

# k = 2
knn350_2 = knnreg(price ~ mileage, data = sclass350_train, k = 2)

sclass350_test = sclass350_test %>%
  mutate(k2_price_pred = predict(knn350_2,sclass350_test))

rmse(knn350_2, sclass350_test)

# k = 10
knn350_10 = knnreg(price ~ mileage, data = sclass350_train, k = 10)

sclass350_test = sclass350_test %>%
  mutate(k10_price_pred = predict(knn350_10,sclass350_test))

rmse(knn350_10, sclass350_test)

# k = 25
knn350_25 = knnreg(price ~ mileage, data = sclass350_train, k = 25)

sclass350_test = sclass350_test %>%
  mutate(k25_price_pred = predict(knn350_25,sclass350_test))

rmse(knn350_25, sclass350_test)

# k = 100
knn350_100 = knnreg(price ~ mileage, data = sclass350_train, k = 100)

sclass350_test = sclass350_test %>%
  mutate(k100_price_pred = predict(knn350_100,sclass350_test))

rmse(knn350_100, sclass350_test)

# k = 300
knn350_300 = knnreg(price ~ mileage, data = sclass350_train, k = 300)

sclass350_test = sclass350_test %>%
  mutate(k300_price_pred = predict(knn350_300,sclass350_test))

rmse(knn350_300, sclass350_test)

```
Above are the RMSE outputs, in order, of arbitrarily selected values of K (2, 10, 25, 100, and 300 respectively).

```{r sClass350 Testing Data, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
sclass350_testtable = sclass350_test %>%
  arrange(mileage) %>%
  head(10)

knitr::kable(sclass350_testtable,col.names = c("Trim", "Mileage", "Price", "k = 2 Prediction","k = 10 Prediction","k = 25 Prediction","k = 100 Prediction","k = 300 Prediction"),
             caption = "Our Testing Data with Price Predictions Based on Differing Values of k (data arranged by mileage")
```

This table shows our testing set with the predictions of each model associated with a  value of k.  For brevity we added only the first 10 observations. 

#### Plotting RMSE vs K
```{r Trim 350s RMSE Based on the Value of K, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# PLOTTING RMSE VS K FOR TRIM 350

rmse_350 = foreach(i = 1:150, .combine='c') %do%{
  knnmod = knnreg(price ~ mileage, data=(sclass350_train), k=i)
  modelr::rmse(knnmod, data=sclass350_test)
}

xval = c(1:150)

rmse_k1 = data.frame(rmse_350, xval)

ggplot(rmse_k1) +
  geom_line(aes(x=xval, y=rmse_350)) +
  labs(title = "Trim 350's RMSE Based on the Value of K", y = "RMSE", x = "Number of k-nearest Neighbors")
```

#### Finding Optimal K and Plotting the Model
```{r Finding Optimal K and Plotting the Model ,echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}

# FINDING THE OPTIMAL VALUE OF k AND STORING IT

kx350 = rmse_k1 %>%
  arrange(rmse_350) %>%
  head(1) %>%
  select(xval)

k350optimal = kx350[1,1]

k350optimal

# PLOTTING THIS MODEL:

knn350_optimal = knnreg(price ~ mileage, data = sclass350_train, k = k350optimal)

sclass350_test = sclass350_test %>%
  mutate(koptimal_price_pred = predict(knn350_optimal, sclass350_test))

p_test1 = ggplot(sclass350_test) +
  geom_point(aes(x = mileage, y = price))

p_test1 + geom_line(aes(x = mileage, y = koptimal_price_pred), color = "red") +
  labs(title = "How Trim 350's price depends on mileage", x = "Mileage", y = "Price")
```

### Trim 65 AMG analysis

#### Making predictions

```{r Splitting the data and making predictions, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# SPLITTING TRIM 65 AMG'S DATASET FOR KNN
sclass65_split = initial_split(sclass65, prop = 0.80)
sclass65_train = training(sclass65_split)
sclass65_test = testing(sclass65_split)

# FOR DIFFERENT LEVELS OF K, MAKING PREDICTIONS ON THE TESTING SET AND CALCULATING THE RMSE #

# k = 2
knn65_2 = knnreg(price ~ mileage, data = sclass65_train, k = 2)

sclass65_test = sclass65_test %>%
  mutate(k2_price_pred = predict(knn65_2, sclass65_test))

rmse(knn65_2, sclass65_test)

# k = 10
knn65_10 = knnreg(price ~ mileage, data = sclass65_train, k = 10)

sclass65_test = sclass65_test %>%
  mutate(k10_price_pred = predict(knn65_10,sclass65_test))

rmse(knn65_10, sclass65_test)

# k = 25
knn65_25 = knnreg(price ~ mileage, data = sclass65_train, k = 25)

sclass65_test = sclass65_test %>%
  mutate(k25_price_pred = predict(knn65_25,sclass65_test))

rmse(knn65_25, sclass65_test)

# k = 100
knn65_100 = knnreg(price ~ mileage, data = sclass65_train, k = 100)

sclass65_test = sclass65_test %>%
  mutate(k100_price_pred = predict(knn65_100, sclass65_test))

rmse(knn65_100, sclass65_test)

# k = 200
knn65_200 = knnreg(price ~ mileage, data = sclass65_train, k = 200)

sclass65_test = sclass65_test %>%
  mutate(k200_price_pred = predict(knn65_200, sclass65_test))

rmse(knn65_200, sclass65_test)
```

Above are the predicted RMSE values for different values of K (2, 10, 25, 100, and 200 respectively).

```{r Class65 Testing Data, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
sclass65_testtable = sclass65_test %>%
  arrange(mileage) %>%
  head(10)

knitr::kable(sclass65_testtable,col.names = c("Trim", "Mileage", "Price", "k = 2 Prediction","k = 10 Prediction","k = 25 Prediction","k = 100 Prediction","k = 200 Prediction"),
             caption = "Our Testing Data with Price Predictions Based on Differing Values of k (data arranged by mileage")
```

This table shows our testing set with the predictions of each model associated with a value of k.  For brevity we added only the first 10 observations.

#### Plotting RMSE vs K
```{r Plotting RMSE vs K, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}

# PLOTTING RMSE VS K FOR TRIM 65 AMG

rmse_65 = foreach(j = 1:100, .combine='c') %do%{
  knnmod2 = knnreg(price ~ mileage, data=(sclass65_train), k=j)
  modelr::rmse(knnmod2, data=sclass65_test)
}

xval2 = c(1:100)

rmse_k2 = data.frame(rmse_65, xval2)

ggplot(rmse_k2) +
  geom_line(aes(x=xval2, y=rmse_65)) +
  labs(title = "Trim 65 AMG's RMSE Based on the Value of K", y = "RMSE", x = "Number of K-Nearest Neighbors")
```


#### Finding Optimal K and Plotting the Model

```{r Finding Optimal K and Plotting the Model, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE}
# FINDING THE OPTIMAL VALUE OF k AND STORING IT

kx65 = rmse_k2 %>%
  arrange(rmse_65) %>%
  head(1) %>%
  select(xval2)

k65optimal = kx65[1,1]

k65optimal

# PLOTTING THIS MODEL:

knn65_optimal = knnreg(price ~ mileage, data = sclass65_train, k = k65optimal)

sclass65_test = sclass65_test %>%
  mutate(koptimal_price_pred = predict(knn65_optimal, sclass65_test))

p_test2 = ggplot(sclass65_test) +
  geom_point(aes(x = mileage, y = price))

p_test2 + geom_line(aes(x = mileage, y = koptimal_price_pred), color = "red") +
  labs(title = "How Trim 65 AMG's price depends on Mileage", x = "Mileage", y = "Price")
```

### Discussion

It would appear that, with repeated trials, Trim 350's optimal value of k is generally higher.  This is due to Trim 350's larger sample size. Generally, higher values of k will reduce the variation of our model but at the cost of greater bias.  With larger sample sizes, we can afford to use larger values of k as these values will have a lesser effect on the model's bias.
